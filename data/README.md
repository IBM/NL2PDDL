
# Data 

This directory contains all data used by the project.

# Contents

* `pddlData (pddl)` : These are the ground truth domains 
(Marked as D in Figure 1 of the paper) and sets of problems for
which ground truth plans (Î _D in the paper) are generated. This also folder
contains a python module for parsing and caching python objects representing
PDDL domains and problems. 

* `domainNL (csv)` : Contains human crafted NL descriptions of each action 
in each domain  (Marked as N(a) in Figure 1 of the paper) for each
description class.

* `promptTemplates (txt)` : Contains templates (fstrings) used generale the prompts. 

* `prompts (json)` : Prompts for the LLM, one for each action, generated by 
instantiating a prompt template in `promptTemplates`. Multiple prompts
are generated for each action with randomized contexts from domains outside
the current one to help normalize results, these results are averaged at
the end.

* `outputs (json)` : Adds raw output data from the LLM for each action prompt 
(Marked as T(a) in Figure 1 of the paper) to the `prompts`

* `parsedOutputs (json)` : Parsed raw outputs from the LLMs. Adds fields to 
`outputs` for syntax and some semantic errors, results in a set of mostly 
valid reconstructed actions (A' in Figure 1). We note that we rely on the K*
translator to catch some semantic errors, so this does not include all 
correctly reconstructed actions.

* `results (json)` : Adds the results of running the metric computation on the
reconstructed domains D' to the `parsedOutputs`.  

* `parsedResults (csv)` : Converted and cleaned `results` as a CSV file used 
for plot generation.


# Data Relations

We make the relations between our data clear here:

* `domainNL` files are handcrafted based ground truth domains in `pddlData`. 

* `prompts` are algorithmically generated from `domainNL` using `promptTemplates`.

* `outputs` are generated by the LLM from `prompts`.

* `parsedOutputs` are algorithmically generated by running a pddl syntax check
on `outputs`.

* `results` are computed by running K* and VAL on reconstructed domains
generated from `parsedOutputs` and comparing them with ground truth domains
from `pddlData`.

* `parsedResults` are algorithmically generated from `results`.

# JSON Schema
The basic task first used for `prompts` each subsequent metric calculation
will add to the `results` felid.
```
{
    "domain" : the name of the domain,
    "action" : the name of the action,
    "pddl" : the pddl representation of the action,
    "class" : the name of the importance class,
    "context" : list of objects stating what domains and actions
                were used for the context
    [{
        "domain" : the domain the context was taken from,
        "action" : the action the context was taken from.
    }],
    "prompt" : the full prompt with context,
    "results" : [] an empty array for future run data on this task
}
```

